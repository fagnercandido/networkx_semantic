{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b1ee07-9fb7-49f4-ac23-aa40d2695940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b965b93a-074c-40df-b976-24a4376105d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based https://alvinntnu.github.io/python-notes/statistical-analyses/network-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4475e764-8c3b-4ad3-a06a-80e018e14cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinonimos = ['sentir', 'pensar', 'reivindicar', 'representar', 'enfatizar', 'mostrar', 'explicar', 'apontar', 'elevar', 'reivindicar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd429fa5-b04a-4f07-a116-931f3acaaa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (len, dim) (500000, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_vectors = nlp.vocab.vectors\n",
    "print('Shape: (len, dim)', words_vectors.shape)\n",
    "len(words_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2940bca-696e-4890-8aea-3deaf023911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = nlp.vocab['pensar']\n",
    "w2 = nlp.vocab['sentir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ac664b-883a-4c5c-8fd6-6f10baef1027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentir</th>\n",
       "      <th>pensar</th>\n",
       "      <th>reivindicar</th>\n",
       "      <th>representar</th>\n",
       "      <th>enfatizar</th>\n",
       "      <th>mostrar</th>\n",
       "      <th>explicar</th>\n",
       "      <th>apontar</th>\n",
       "      <th>elevar</th>\n",
       "      <th>reivindicar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentir</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pensar</th>\n",
       "      <td>0.66</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reivindicar</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>representar</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfatizar</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mostrar</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explicar</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apontar</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elevar</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reivindicar</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sentir  pensar  reivindicar  representar  enfatizar  mostrar  \\\n",
       "sentir         1.00    0.66         0.31         0.34       0.41     0.55   \n",
       "pensar         0.66    1.00         0.43         0.42       0.58     0.62   \n",
       "reivindicar    0.31    0.43         1.00         0.70       0.62     0.54   \n",
       "representar    0.34    0.42         0.70         1.00       0.68     0.67   \n",
       "enfatizar      0.41    0.58         0.62         0.68       1.00     0.74   \n",
       "mostrar        0.55    0.62         0.54         0.67       0.74     1.00   \n",
       "explicar       0.49    0.67         0.55         0.61       0.75     0.79   \n",
       "apontar        0.43    0.57         0.60         0.66       0.72     0.76   \n",
       "elevar         0.38    0.38         0.50         0.58       0.58     0.56   \n",
       "reivindicar    0.31    0.43         1.00         0.70       0.62     0.54   \n",
       "\n",
       "             explicar  apontar  elevar  reivindicar  \n",
       "sentir           0.49     0.43    0.38         0.31  \n",
       "pensar           0.67     0.57    0.38         0.43  \n",
       "reivindicar      0.55     0.60    0.50         1.00  \n",
       "representar      0.61     0.66    0.58         0.70  \n",
       "enfatizar        0.75     0.72    0.58         0.62  \n",
       "mostrar          0.79     0.76    0.56         0.54  \n",
       "explicar         1.00     0.75    0.47         0.55  \n",
       "apontar          0.75     1.00    0.51         0.60  \n",
       "elevar           0.47     0.51    1.00         0.50  \n",
       "reivindicar      0.55     0.60    0.50         1.00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pairwise_similarity(word_list, nlp):\n",
    "    word_sim_matrix = np.ones(shape = (len(word_list), len(word_list)))\n",
    "    for i, w1 in enumerate(word_list):\n",
    "        for j, w2 in enumerate(word_list):\n",
    "            if w1 !=w2:\n",
    "                word_sim_matrix[i,j] = nlp.vocab[str(w1)].similarity(nlp.vocab[str(w2)])\n",
    "    return(word_sim_matrix)\n",
    "        \n",
    "pd.DataFrame(data = np.round(pairwise_similarity(sinonimos, nlp), 2),\n",
    "             index = sinonimos,\n",
    "             columns = sinonimos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f973af-d233-4bdb-b0cd-c9391f0d164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665468\n",
      "['2.049', '2.05', '2.050', '2.051', '2.052', '2.053', '2.055', '2.056', '2.057', '2.058', '2.059', '2.06', '2.060', '2.061', '2.062', '2.063', '2.064', '2.065', '2.066', '2.068', '2.069', '2.07', '2.070', '2.071', '2.072', '2.073', '2.074', '2.075', '2.076', '2.077', '2.078', '2.079', '2.08', '2.080', '2.081', '2.082', '2.083', '2.084', '2.085', '2.086', '2.087', '2.088', '2.089', '2.090', '2.091', '2.092', '2.093', '2.094', '2.095', '2.096', '2.097', '2.099', '2.0GHz', '2.0L', '2.0MP', '2.0c', '2.0ghz', '2.0l', '2.0mp', '2.1', '2.1,2', '2.1-', '2.1.0', '2.1.1', '2.1.2', '2.1.3', '2.1.4', '2.1.5', '2.1.6', '2.1/2', '2.10', '2.100', '2.100,00', '2.100.000,00', '2.101', '2.105', '2.105.441', '2.107', '2.108', '2.109', '2.109.049', '2.11', '2.110', '2.111', '2.112', '2.114', '2.115', '2.117', '2.119', '2.12', '2.120', '2.121', '2.124', '2.125', '2.126', '2.129', '2.13', '2.130', '2.131', '2.132', '2.133', '2.134', '2.135', '2.135,64', '2.136', '2.14', '2.140', '2.142', '2.145', '2.148', '2.149', '2.15', '2.150', '2.150.097', '2.151', '2.152', '2.153', '2.154', '2.158', '2.159', '2.16', '2.160', '2.165', '2.166', '2.168', '2.17', '2.170', '2.172', '2.172/97', '2.174', '2.175', '2.175,17', '2.176', '2.176,26', '2.178', '2.18', '2.180', '2.181', '2.181/97', '2.182', '2.184', '2.185', '2.186', '2.187', '2.19', '2.190', '2.191', '2.192', '2.195', '2.196', '2.197', '2.198', '2.199', '2.1A', '2.1a', '2.2', '2.2-', '2.2.', '2.2.1', '2.2.2', '2.2.3', '2.2.4', '2.2.5', '2.2.6', '2.2.7', '2.20', '2.200', '2.200,00', '2.200.000', '2.200.000,00', '2.201', '2.202', '2.204', '2.205', '2.206', '2.208', '2.209', '2.21', '2.210', '2.212', '2.215', '2.219', '2.22', '2.220', '2.221', '2.222', '2.222.725', '2.223', '2.224', '2.225', '2.226', '2.227', '2.228', '2.23', '2.230', '2.231', '2.232', '2.236,30', '2.237', '2.24']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(nlp.vocab.strings)\n",
    "print(len(vocab))\n",
    "print(vocab[20000:20200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7cef4f-50f8-42a7-9cfc-ab36fd59211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 103 ms, total: 14.4 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "target_word = 'sentir'\n",
    "word_sim = []\n",
    "\n",
    "target_word_vocab = nlp.vocab[target_word]\n",
    "for w in vocab:\n",
    "    w_vocab = nlp.vocab[w]\n",
    "    if w_vocab.vector is not None and np.count_nonzero(w_vocab.vector) and not w_vocab.is_ascii and not w_vocab.is_punct and w != target_word:\n",
    "        word_sim.append((w, target_word_vocab.similarity(w_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ee78fd-d9b1-4ca2-8531-a706cc2e9922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentirás', 0.7892873883247375),\n",
       " ('sentirá', 0.7855808138847351),\n",
       " ('sentí-la', 0.7750628590583801),\n",
       " ('sentí-lo', 0.7662668824195862),\n",
       " ('sentí', 0.7393266558647156),\n",
       " ('percebê-la', 0.7342424988746643),\n",
       " ('percebê-lo', 0.7295681238174438),\n",
       " ('sentir-se-ão', 0.7095298171043396),\n",
       " ('sentir-se-á', 0.7068419456481934),\n",
       " ('sentirão', 0.7067611217498779)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_sim, key = lambda x : x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a2c847-2047-4e8a-aae3-732f09d93fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "w1 = nlp.vocab['sentir']\n",
    "w2 = nlp.vocab['navio']\n",
    "\n",
    "print(w2.is_ascii)\n",
    "print(w2.is_currency)\n",
    "print(w2.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0720cf7-cdc0-4b6a-884f-73d7b59b1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def cosine_similarity_numba(u:np.ndarray, v:np.ndarray):\n",
    "    assert(u.shape[0] == v.shape[0])\n",
    "    uv = 0\n",
    "    uu = 0\n",
    "    vv = 0\n",
    "    for i in range(u.shape[0]):\n",
    "        uv += u[i]*v[i]\n",
    "        uu += u[i]*u[i]\n",
    "        vv += v[i]*v[i]\n",
    "    cos_theta = 1\n",
    "    if uu != 0 and vv != 0:\n",
    "        cos_theta = uv/np.sqrt(uu*vv)\n",
    "    return cos_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f683d8-267b-4945-b3d8-34d458fac9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_v1(word, topn = 5):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [\n",
    "          w for w in nlp.vocab \n",
    "          if np.count_nonzero(w.vector) and not w.is_ascii and not w.is_punct and len(w.text) == 2\n",
    "    ]\n",
    "  \n",
    "    by_similarity = sorted(queries, key = lambda w: cosine_similarity_numba(w.vector, word.vector), reverse = True)\n",
    "    return [(w.text,w.similarity(word)) for w in by_similarity[:topn+1] if w.text != word.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "228b02c9-2a59-4003-af18-2b5dd99c6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_v2(word, topn=5):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [\n",
    "      w for w in nlp.vocab \n",
    "      if np.count_nonzero(w.vector) and not w.is_ascii and not w.is_punct and len(w.text)==2\n",
    "    ]\n",
    "\n",
    "    by_similarity = sorted(queries, key = lambda w: word.similarity(w), reverse = True)\n",
    "    return [(w.text,w.similarity(word)) for w in by_similarity[:topn+1] if w.text != word.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "194f8fa4-2d9f-42e5-b0a0-d2f770a58de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 6.98 ms, total: 2.11 s\n",
      "Wall time: 2.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('só', 0.32476431131362915),\n",
       " ('vê', 0.28149497509002686),\n",
       " ('pé', 0.22125814855098724),\n",
       " ('fé', 0.21561820805072784)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "most_similar_v1(\"sentir\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34fc0873-1acd-43ee-83d3-6e36dbc6f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 4 ms, total: 2.1 s\n",
      "Wall time: 2.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('só', 0.32476431131362915),\n",
       " ('vê', 0.28149497509002686),\n",
       " ('pé', 0.22125814855098724),\n",
       " ('fé', 0.21561820805072784)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "most_similar_v2(\"sentir\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcde307b-c3cf-4f58-b115-1e26e1dd9866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 s, sys: 29.6 ms, total: 21.2 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sinonimos_topn = dict([(w, most_similar_v1(w, topn=1000)) for w in sinonimos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3030841b-73ca-41b5-bccb-6df3c815ca34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('só', 0.32476431131362915),\n",
       " ('vê', 0.28149497509002686),\n",
       " ('pé', 0.22125814855098724),\n",
       " ('fé', 0.21561820805072784),\n",
       " ('lá', 0.19053611159324646),\n",
       " ('aí', 0.1902761608362198),\n",
       " ('já', 0.17124590277671814),\n",
       " ('Vê', 0.1457124948501587),\n",
       " ('és', 0.1431615650653839),\n",
       " ('dá', 0.13862080872058868)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinonimos_topn[sinonimos[0]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba6588a4-dd94-412c-86e4-2a97bc9f603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinonimos_topn_list = []\n",
    "for w, s in sinonimos_topn.items():\n",
    "    for s_w, s_s in s:\n",
    "        sinonimos_topn_list.append((w, s_w, s_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fc460e0-deda-4e75-bf7d-8808c64ba83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sentir', 'só', 0.32476431131362915), ('sentir', 'vê', 0.28149497509002686), ('sentir', 'pé', 0.22125814855098724), ('sentir', 'fé', 0.21561820805072784), ('sentir', 'lá', 0.19053611159324646), ('sentir', 'aí', 0.1902761608362198), ('sentir', 'já', 0.17124590277671814), ('sentir', 'Vê', 0.1457124948501587), ('sentir', 'és', 0.1431615650653839), ('sentir', 'dá', 0.13862080872058868)]\n",
      "9009\n"
     ]
    }
   ],
   "source": [
    "print(sinonimos_topn_list[:10])\n",
    "print(len(sinonimos_topn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "102da330-a156-4686-9b83-44d8653cbc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentir</td>\n",
       "      <td>só</td>\n",
       "      <td>0.324764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentir</td>\n",
       "      <td>vê</td>\n",
       "      <td>0.281495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentir</td>\n",
       "      <td>pé</td>\n",
       "      <td>0.221258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentir</td>\n",
       "      <td>fé</td>\n",
       "      <td>0.215618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentir</td>\n",
       "      <td>lá</td>\n",
       "      <td>0.190536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9004</th>\n",
       "      <td>elevar</td>\n",
       "      <td>גם</td>\n",
       "      <td>-0.126297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9005</th>\n",
       "      <td>elevar</td>\n",
       "      <td>ín</td>\n",
       "      <td>-0.127578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9006</th>\n",
       "      <td>elevar</td>\n",
       "      <td>их</td>\n",
       "      <td>-0.127596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9007</th>\n",
       "      <td>elevar</td>\n",
       "      <td>vú</td>\n",
       "      <td>-0.127742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008</th>\n",
       "      <td>elevar</td>\n",
       "      <td>”3</td>\n",
       "      <td>-0.128827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9009 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          w1  w2       sim\n",
       "0     sentir  só  0.324764\n",
       "1     sentir  vê  0.281495\n",
       "2     sentir  pé  0.221258\n",
       "3     sentir  fé  0.215618\n",
       "4     sentir  lá  0.190536\n",
       "...      ...  ..       ...\n",
       "9004  elevar  גם -0.126297\n",
       "9005  elevar  ín -0.127578\n",
       "9006  elevar  их -0.127596\n",
       "9007  elevar  vú -0.127742\n",
       "9008  elevar  ”3 -0.128827\n",
       "\n",
       "[9009 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(sinonimos_topn_list, columns = ['w1','w2','sim'])\n",
    "df[df['sim'] > 0.6]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "560db726-c3e8-4520-be72-2644093a41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_SIMILARITY_CUTOFF = 0.05\n",
    "df2 = df[df['sim'] > WORD_SIMILARITY_CUTOFF]\n",
    "nodes_id = list(set(list(df2['w2'].values) + list(df2['w1'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46ccb932-b85a-49b7-86a0-9602c1f9910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473\n",
      "(2936, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes_id))\n",
    "m = len(nodes_id)\n",
    "distances = np.zeros((m,m))\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(m):  \n",
    "        distances[i,j] = nlp.vocab[nodes_id[i]].similarity(nlp.vocab[nodes_id[j]])\n",
    "\n",
    "EMBEDDING_CUTOFF = 0.75\n",
    "\n",
    "distances_flat = []\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        if distances[i,j]> EMBEDDING_CUTOFF and i != j:\n",
    "            distances_flat.append((nodes_id[i], nodes_id[j], distances[i,j]))\n",
    "\n",
    "edges_df = pd.DataFrame(distances_flat, columns=['w1','w2','sim'])\n",
    "print(edges_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef33a067-8f7b-409a-bc0e-98b9dbfce1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4091, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>لم</td>\n",
       "      <td>أن</td>\n",
       "      <td>0.922549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لم</td>\n",
       "      <td>پس</td>\n",
       "      <td>0.825348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لم</td>\n",
       "      <td>هر</td>\n",
       "      <td>0.833947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لم</td>\n",
       "      <td>تا</td>\n",
       "      <td>0.828287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>لم</td>\n",
       "      <td>فى</td>\n",
       "      <td>0.904688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>elevar</td>\n",
       "      <td>µM</td>\n",
       "      <td>0.051421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8077</th>\n",
       "      <td>elevar</td>\n",
       "      <td>ﬁm</td>\n",
       "      <td>0.050826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8078</th>\n",
       "      <td>elevar</td>\n",
       "      <td>Yū</td>\n",
       "      <td>0.050798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8079</th>\n",
       "      <td>elevar</td>\n",
       "      <td>só</td>\n",
       "      <td>0.050298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8080</th>\n",
       "      <td>elevar</td>\n",
       "      <td>小说</td>\n",
       "      <td>0.050238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4091 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          w1  w2       sim\n",
       "0         لم  أن  0.922549\n",
       "1         لم  پس  0.825348\n",
       "2         لم  هر  0.833947\n",
       "3         لم  تا  0.828287\n",
       "4         لم  فى  0.904688\n",
       "...      ...  ..       ...\n",
       "8076  elevar  µM  0.051421\n",
       "8077  elevar  ﬁm  0.050826\n",
       "8078  elevar  Yū  0.050798\n",
       "8079  elevar  só  0.050298\n",
       "8080  elevar  小说  0.050238\n",
       "\n",
       "[4091 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df = edges_df.append(df2).drop_duplicates()\n",
    "print(edges_df.shape)\n",
    "edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c40f642c-41e7-486c-889c-becd71f348c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9958aac-b3c3-487c-8a7b-449d171acf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myRescaler(x):\n",
    "    x = np.array(x)\n",
    "    y = np.interp(x, (x.min(), x.max()), (5, 20))\n",
    "    return list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2574cb0-e386-43a5-8511-d75cb7a1e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G= nx.from_pandas_edgelist(edges_df, 'w1','w2','sim')\n",
    "\n",
    "nodes_df = pd.DataFrame({'id':list(G.nodes),\n",
    "                         'betweenness': myRescaler(list(nx.betweenness_centrality(G).values())),\n",
    "                         'eigenvector': myRescaler(list(nx.eigenvector_centrality(G).values()))})\n",
    "nodes_df['size']=[5 if i not in sinonimos else 10 for i in nodes_id]\n",
    "nodes_df['size2']= [i if i not in sinonimos else 30 for i in nodes_df['eigenvector']]\n",
    "nodes_df['group'] = ['KEY' if nodes_df.loc[i,'id'] in sinonimos else 'CONTEXT' for i in range(nodes_df.shape[0])]\n",
    "nodes_df['color'] = ['lightpink' if nodes_df.loc[i,'group']=='KEY' else 'lightblue' for i in range(nodes_df.shape[0])]\n",
    "nodes_df['borderWidthSelected'] = list(np.repeat(20.0, nodes_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a73db292-2f93-4703-a27f-69d189340cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gvis = Network(\"768px\",\"1600px\", notebook=False,heading=\"Semantic Network\")\n",
    "edges_in = list(edges_df.to_records(index=False))\n",
    "\n",
    "for i in range(nodes_df.shape[0]):\n",
    "    Gvis.add_node(list(G.nodes)[i], value=nodes_df.loc[i,'size2'], group=nodes_df.loc[i,'group'])#, color=nodes_df.loc[i,'color'], borderWidthSelected = nodes_df.loc[i,'borderWidthSelected'])\n",
    "\n",
    "Gvis.add_edges(edges_in)\n",
    "#Gvis.show_buttons()\n",
    "Gvis.set_options(\"\"\"\n",
    "  var options = {\n",
    "    \"nodes\": {\n",
    "      \"borderWidth\": 0,\n",
    "      \"color\": {\n",
    "        \"highlight\": {\n",
    "          \"border\": \"rgba(221,171,197,1)\",\n",
    "          \"background\": \"rgba(248,178,255,1)\"\n",
    "        }\n",
    "      },\n",
    "      \"shadow\": {\n",
    "        \"enabled\": true\n",
    "      }\n",
    "    },\n",
    "    \"edges\": {\n",
    "      \"color\": {\n",
    "        \"highlight\": \"rgba(255,192,200,1)\",\n",
    "        \"inherit\": false\n",
    "      },\n",
    "      \"smooth\": false\n",
    "    },\n",
    "    \"interaction\": {\n",
    "      \"hover\": true,\n",
    "      \"navigationButtons\": true\n",
    "    },\n",
    "    \"manipulation\": {\n",
    "      \"enabled\": true\n",
    "    },\n",
    "    \"physics\": {\n",
    "      \"barnesHut\": {\n",
    "        \"springLength\": 270\n",
    "      },\n",
    "      \"minVelocity\": 0.75\n",
    "    }\n",
    "  }\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bea16ba-b27a-4342-874f-2df0df72cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gvis.show('Gvis.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e8db5-22cc-413d-bb5c-f76e6d64b67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
